{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement: Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this](https://mrq02.github.io/DL/imp/#gc) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) **1D GC**\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> <u><b>Figure 1</b></u>: <b>1D linear model</b><br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(x,theta):\n",
    "    return theta*x\n",
    "\n",
    "def backprop(x,theta):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x,theta,epsilon=1e-7):\n",
    "    # Compute the approx gradient\n",
    "    thetap = theta+epsilon\n",
    "    thetam = theta-epsilon\n",
    "    Jp = forwardprop(x,thetap)\n",
    "    Jm = forwardprop(x,thetam)\n",
    "    gradapprox = (Jp-Jm)/(2*epsilon)\n",
    "    \n",
    "    # Compute the actual gradient\n",
    "    grad = backprop(x,theta)\n",
    "    \n",
    "    # Compute the diff between the 2 gradients\n",
    "    num = np.linalg.norm(grad-gradapprox)\n",
    "    denom = np.linalg.norm(grad)+np.linalg.norm(gradapprox)\n",
    "    diff = num/denom\n",
    "    if diff<1e-7:\n",
    "        print(\"The gradient is correct.\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong.\")\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct.\n",
      "diff = 1.2623786476544242e-09\n"
     ]
    }
   ],
   "source": [
    "x, theta = 10, 8\n",
    "diff = gradient_check(x,theta)\n",
    "print(\"diff = \"+str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **N-D GC**\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u><b>Figure 2</b></u>: <b>deep neural network</b><br><i>LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID</i></center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_n(X,Y,params):\n",
    "    # Get the params\n",
    "    m = X.shape[1]\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    W3 = params[\"W3\"]\n",
    "    b3 = params[\"b3\"]\n",
    "\n",
    "    # Forward\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.maximum(0,Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = np.maximum(0,Z2)\n",
    "    Z3 = np.dot(W3,A2)+b3\n",
    "    A3 = 1/(1+np.exp(-Z3))\n",
    "    \n",
    "    # Cost\n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1-A3),1-Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    cache = (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3)\n",
    "    return cost,cache\n",
    "\n",
    "def backprop_n(X,Y,cache):\n",
    "    # Get the params\n",
    "    m = X.shape[1]\n",
    "    (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3) = cache\n",
    "    \n",
    "    # Backward\n",
    "    dZ3 = A3-Y\n",
    "    dW3 = 1./m*np.dot(dZ3,A2.T)\n",
    "    db3 = 1./m*np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    dZ2 = np.multiply(dA2,np.int64(A2>0))\n",
    "    dW2 = 1./m*np.dot(dZ2,A1.T)\n",
    "    db2 = 1./m*np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = np.multiply(dA1,np.int64(A1>0))\n",
    "    dW1 = 1./m*np.dot(dZ1,X.T)\n",
    "    db1 = 1./m*np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Return\n",
    "    grads = {\n",
    "        \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "        \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "        \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1\n",
    "    }\n",
    "    return grads\n",
    "\n",
    "def gradient_check_n_test_case(): \n",
    "    np.random.seed(1)\n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    return x, y, parameters\n",
    "\n",
    "def dict_to_vec(params):\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\"]:\n",
    "        new_vector = np.reshape(params[key],(-1,1))\n",
    "        keys += [key]*new_vector.shape[0]\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta,new_vector),axis=0)\n",
    "        count = count+1\n",
    "    return theta, keys\n",
    "\n",
    "def vec_to_dict(theta):\n",
    "    params = {}\n",
    "    params[\"W1\"] = theta[:20].reshape((5,4))\n",
    "    params[\"b1\"] = theta[20:25].reshape((5,1))\n",
    "    params[\"W2\"] = theta[25:40].reshape((3,5))\n",
    "    params[\"b2\"] = theta[40:43].reshape((3,1))\n",
    "    params[\"W3\"] = theta[43:46].reshape((1,3))\n",
    "    params[\"b3\"] = theta[46:47].reshape((1,1))\n",
    "    return params\n",
    "\n",
    "def grads_to_vec(grads):\n",
    "    count = 0\n",
    "    for key in [\"dW1\",\"db1\",\"dW2\",\"db2\",\"dW3\",\"db3\"]:\n",
    "        new_vector = np.reshape(grads[key],(-1,1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta,new_vector),axis=0)\n",
    "        count = count+1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(params,grads,X,Y,epsilon=1e-7):\n",
    "    # Get the values\n",
    "    params_vec, _ = dict_to_vec(params)\n",
    "    grad = grads_to_vec(grads)\n",
    "    n_params = params_vec.shape[0]\n",
    "    Jp = np.zeros((n_params,1))\n",
    "    Jm = np.zeros((n_params,1))\n",
    "    gradapprox = np.zeros((n_params,1))\n",
    "    \n",
    "    # Compute the approx gradient\n",
    "    for i in range(n_params):\n",
    "        thetap = np.copy(params_vec)\n",
    "        thetap[i][0] += epsilon\n",
    "        Jp[i], _ = forwardprop_n(X,Y,vec_to_dict(thetap))\n",
    "        thetam = np.copy(params_vec)\n",
    "        thetam[i][0] -= epsilon\n",
    "        Jm[i], _ = forwardprop_n(X,Y,vec_to_dict(thetam))\n",
    "        gradapprox[i] = (Jp[i]-Jm[i])/(2*epsilon)\n",
    "    \n",
    "    # Compute the diff between the 2 gradients\n",
    "    num = np.linalg.norm(grad-gradapprox)\n",
    "    denom = np.linalg.norm(grad)+np.linalg.norm(gradapprox)\n",
    "    diff = num/denom\n",
    "    if diff<1e-7:\n",
    "        print(\"The gradient is correct.\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong.\")\n",
    "    print(\"diff = \"+str(diff))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is wrong.\n",
      "diff = 1.1885552035482147e-07\n"
     ]
    }
   ],
   "source": [
    "X,Y,params = gradient_check_n_test_case()\n",
    "cost,cache = forwardprop_n(X,Y,params)\n",
    "grads = backprop_n(X,Y,cache)\n",
    "diff = gradient_check_n(params,grads,X,Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
